apiVersion: v1
kind: Pod
metadata:
  name: mlperf-inference-gpt
spec:
  restartPolicy: Never
  containers:
  - name: mlperf-env
    image: quay.io/meyceoz/mlperf-inference-gpt:vllm-fix-offline
    resources:
      requests:
        cpu: 96
        memory: 1024Gi
        nvidia.com/gpu: 8
      limits:
        cpu: 96
        memory: 1024Gi
        nvidia.com/gpu: 8
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
    command: [ "/bin/sh", "-c" ]
    args: [ "sleep infinity" ]
  volumes:
  - name: dshm
    emptyDir:
      medium: Memory