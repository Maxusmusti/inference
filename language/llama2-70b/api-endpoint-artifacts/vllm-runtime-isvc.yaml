apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/template-display-name: ServingRuntime for vLLM | Topsail
  labels:
    opendatahub.io/dashboard: "true"
  name: llama-2-70b-chat-hf-vllm-servingruntime
  namespace: llama
spec:
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
  - args:
    - --model=/mnt/models/
    - --download-dir=/models-cache
    - --port=8080
      #      - --dtype=fp8
      #- --kv-cache-dtype=fp8
    - --tensor-parallel-size=4
    - --max-model-len=2048
    - --enable-chunked-prefill
    - --gpu-memory-utilization=0.95
    - --max-num-seqs 512
    image: quay.io/wxpe/tgis-vllm:2a60ad3
    name: kserve-container
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP
    volumeMounts:
    - mountPath: /home/vllm
      name: home
    - mountPath: /.cache
      name: cache
    - mountPath: /.config
      name: config
  resources:
    limits:
      nvidia.com/gpu: "4"
    requests:
      cpu: "8"
      memory: 40gi
      nvidia.com/gpu: "4"
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: pytorch
  volumes:
  - emptyDir: {}
    name: home
  - emptyDir: {}
    name: cache
  - emptyDir: {}
    name: config
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/enable-prometheus-scraping: "true"
  labels:
    opendatahub.io/dashboard: "true"
  name: llama-2-70b-chat-hf-isvc
  namespace: llama
spec:
  predictor:
    minReplicas: 1
    model:
      modelFormat:
        name: pytorch
      
      runtime: llama-2-70b-chat-hf-vllm-servingruntime
      storageUri: pvc://model-storage/Llama-2-70b-chat-hf-fp8
      volumeMounts:
      - mountPath: /dev/shm
        name: shared-memory
      - mountPath: /tmp
        name: tmp
      - mountPath: /home/vllm
        name: home
          #    serviceAccountName: sa
    volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 32Gi
      name: shared-memory
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: home
